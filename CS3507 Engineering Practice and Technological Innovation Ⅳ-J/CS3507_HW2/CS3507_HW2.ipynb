{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D0DIzFeRB0T"
   },
   "source": [
    "这是 **工程实践与科技创新Ⅳ-J (CS3507)** 的第二次课程作业，请仔细阅读以下注意事项：\n",
    "\n",
    "1. 注意作业截止日期，提交作业只需上传 `ipynb` 文件即可，<font color=red>请务必保留所有单元格的运行结果！</font>\n",
    "2. 本次提供了少量参考代码，大部分可直接使用也可自行修改，不能更改或需要更改处会标注\n",
    "3. 本次作业分三部分，三部分得分占比约50%，25%，25% ，请留意2、3部分有一定的问答\n",
    "4. 建议本地补全并运行，保留自己相应的结果"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u4YrnVzFlyOC"
   },
   "source": [
    "在本次作业中，我们将实现与Attention相关的几个模块。\n",
    "\n",
    "Task1: 实现自注意力模块的前向过程与反向过程，最终输出给定输入相应的output, d_W_q, d_W_k, d_W_v\n",
    "\n",
    "一些提示：对于注意力机制还不太熟悉的同学可以回顾原文https://arxiv.org/abs/1706.03762\n",
    "或是一些优秀专栏https://www.zhihu.com/tardis/bd/art/414084879?source_id=1001 加深对其的了解。\n",
    "关键其实就是实现相应公式，再反向求梯度。\n",
    "<center><img src=\"attention.png\"/></center>\n",
    "\n",
    "反向求梯度推导过程：\n",
    "$$\n",
    "\\begin{align*}\n",
    "& 设输入矩阵为X. \\\\\n",
    "& \\left\\{\\begin{array}{l} \n",
    "Q = X\\cdot W_Q, K = X\\cdot W_K, V=X\\cdot W_V \\\\\n",
    "Score = Q\\cdot K^T \\\\\n",
    "Temp = \\frac{Score}{\\sqrt{d_K}} \\\\\n",
    "W_{Score} = softmax(Temp) \\\\\n",
    "Context = W_{Score}\\cdot V \n",
    "\\end{array}\\right.\\\\\n",
    "\n",
    "\n",
    "& 由求梯度公式：&\\\\\n",
    "& Y = X\\cdot W \\Rightarrow \n",
    "\\left\\{\\begin{array}{l} \n",
    "dW = X^T\\cdot dY \\\\\n",
    "dX = dY\\cdot W^T \n",
    "\\end{array}\\right.\\\\\n",
    "\n",
    "& 可得：\\\\\n",
    "& \\left\\{\\begin{array}{l} \n",
    "dW_V = X^T\\cdot dV \\\\\n",
    "dV = W_{Score}^T\\cdot dContext\n",
    "\\end{array}\\right.\n",
    "\\Rightarrow dW_V = X^T\\cdot (W_{Score}^T\\cdot dContext) \\\\\n",
    "\n",
    "& \\left\\{\\begin{array}{l} \n",
    "dW_Q = X^T\\cdot dQ \\\\\n",
    "dQ = dScore\\cdot (K^T)^T = dScore\\cdot K \\\\\n",
    "dScore = \\frac{dTemp}{\\sqrt{d}} \\\\\n",
    "dTemp = softmax'(Temp) * dW_{Score} \\\\\n",
    "dW_{Score} = dContext\\cdot V^T \n",
    "\\end{array}\\right.\n",
    "\\Rightarrow dW_Q = X^T\\cdot (\\frac{softmax'(Temp)}{\\sqrt{d}} * (dContext\\cdot V^T))\\cdot K \\\\\n",
    "\n",
    "& \\left\\{\\begin{array}{l} \n",
    "dW_K = X^T\\cdot dK \\\\\n",
    "dK = E\\cdot dK^T \\\\\n",
    "dK^T = Q^T\\cdot dScore \\\\\n",
    "dScore = ...\n",
    "\\end{array}\\right.\n",
    "\\Rightarrow dW_K = X^T\\cdot Q^T\\cdot (\\frac{softmax'(Temp)}{\\sqrt{d}} * (dContext\\cdot V^T)) &\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u9KFxSVAlyOD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出:\n",
      "[[-0.79341887 -0.45775989 -0.81055327]\n",
      " [-0.78585269 -0.452197   -0.80554836]\n",
      " [-0.7782951  -0.44664042 -0.80054912]]\n",
      "W_q 的梯度:\n",
      "[[-0.02401657  2.01914624  1.01479483]\n",
      " [-0.0300841   2.52596572  1.26955251]\n",
      " [-0.03615163  3.0327852   1.5243102 ]]\n",
      "W_k 的梯度:\n",
      "[[-0.07233604 -0.16692219 -0.25532755]\n",
      " [-0.14370815 -0.33175139 -0.50768981]\n",
      " [-0.21508025 -0.49658058 -0.76005207]]\n",
      "W_v 的梯度:\n",
      "[[1.16983906 1.16983906 1.16983906]\n",
      " [1.46983906 1.46983906 1.46983906]\n",
      " [1.76983906 1.76983906 1.76983906]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 你应该会需要用到softmax和它的导数，可以直接用下面定义好的，或者自己实现。\n",
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    softmax_x = exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    return softmax_x\n",
    "\n",
    "def softmax_derivative(x, axis=-1):\n",
    "    softmax_x = softmax(x, axis)\n",
    "    softmax_derivative_x = softmax_x * (1 - softmax_x)\n",
    "    \n",
    "    return softmax_derivative_x\n",
    "\n",
    "def self_attention(input_sequence):\n",
    "    \n",
    "    # 通过np随机初始化权重矩阵\n",
    "    W_q = np.random.randn(3, 3)   # embedding_dim,attention_dim 3,3\n",
    "    W_k = np.random.randn(3, 3)\n",
    "    W_v = np.random.randn(3, 3)\n",
    "\n",
    "    ##############################################\n",
    "    # TODO\n",
    "    # 实现前向过程与反向过程\n",
    "    ##############################################\n",
    "\n",
    "    # 计算Q、K、V\n",
    "    Q = np.dot(input_sequence, W_q)\n",
    "    K = np.dot(input_sequence, W_k)\n",
    "    V = np.dot(input_sequence, W_v)\n",
    "    \n",
    "    # 计算注意力分数\n",
    "    Score = np.dot(Q, K.T)\n",
    "    \n",
    "    # 计算注意力权重\n",
    "    sqrt_d_k = K.shape[1]**0.5\n",
    "    Temp = Score / sqrt_d_k\n",
    "    W_score = softmax(Temp)\n",
    "    \n",
    "    # 计算上下文向量，即公式结果\n",
    "    context = np.dot(W_score, V)\n",
    "    \n",
    "    # 计算中间过程梯度\n",
    "    d_context = np.ones_like(context)  # 上下文向量的梯度，可理解为将context中所有元素求和作为结果，对其中每个元素的梯度都为1\n",
    "\n",
    "    # 计算权重矩阵的梯度\n",
    "    d_W_q = np.dot(input_sequence.T, np.dot(softmax_derivative(Temp) / sqrt_d_k * np.dot(d_context, V.T), K))\n",
    "    d_W_k = np.dot(input_sequence.T, np.dot(Q.T, softmax_derivative(Temp) / sqrt_d_k * np.dot(d_context, V.T)))\n",
    "    d_W_v = np.dot(input_sequence.T, np.dot(W_score.T, d_context))\n",
    "\n",
    "    ##############################################\n",
    "    # 结束您的代码\n",
    "    ##############################################\n",
    "\n",
    "    return context, d_W_q, d_W_k, d_W_v\n",
    "\n",
    "\n",
    "# 以下部分无需修改\n",
    "# 输入\n",
    "input_sequence = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])  #无实际意义\n",
    "\n",
    "# 计算自注意力和梯度\n",
    "output, d_W_q, d_W_k, d_W_v = self_attention(input_sequence)\n",
    "\n",
    "print(\"输出:\")\n",
    "print(output)\n",
    "print(\"W_q 的梯度:\")\n",
    "print(d_W_q)\n",
    "print(\"W_k 的梯度:\")\n",
    "print(d_W_k)\n",
    "print(\"W_v 的梯度:\")\n",
    "print(d_W_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8Myq-1tlyOE"
   },
   "source": [
    "Task2: 实现正余弦位置编码。\n",
    "位置编码（Positional Encoding）在前面的文章（ https://arxiv.org/abs/1706.03762 ）中被引入，可以帮助模型能够更好地捕捉到序列中不同位置之间的关系，从而更准确地理解和表示输入序列。\n",
    "这里我们采用原文中的正余弦位置编码\n",
    "<center><img src=\"PE.png\"/></center>\n",
    "\n",
    "完成之后思考并回答，为什么在Attention中开始引入位置编码，为什么最初采用这种正余弦编码的方式。\n",
    "\n",
    "回答：\n",
    "+ 在注意力机制中引入位置编码是因为传统的注意力机制（比如基于点积的自注意力机制）缺乏位置信息。在自然语言处理等任务中，单词的顺序对理解文本至关重要。因此，为了使模型能够考虑到单词的位置信息，就需要向输入中引入位置编码。它们以某种方式（比如使用正弦和余弦函数）编码单词在句子中的位置信息。这样，模型就可以根据单词的位置来调整其在注意力机制中的权重，从而更好地理解输入的顺序信息。\n",
    "+ 正弦和余弦函数具有不同频率和相位，可以编码不同位置的信息。它们具有周期性，能够捕捉到不同位置之间的相对位置信息。这种编码方式可以将位置信息嵌入到模型中，帮助模型在学习中更好地理解不同位置之间的关系，而且相对简单易于计算，能提高模型在处理序列数据时的性能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aEZkM-HX88st"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{12: [-0.5365729180004349, -0.5365729180004349, 0.5286341178588567, 0.5286341178588567, 0.02585033637662907, 0.02585033637662907], 1: [0.8414709848078965, 0.8414709848078965, 0.046399223464731285, 0.046399223464731285, 0.0021544330233656045, 0.0021544330233656045], 8: [0.9893582466233818, 0.9893582466233818, 0.3628524110177816, 0.3628524110177816, 0.017234624199596284, 0.017234624199596284], 9: [0.4121184852417566, 0.4121184852417566, 0.40569856994848597, 0.40569856994848597, 0.019388697233126855, 0.019388697233126855], 3: [0.1411200080598672, 0.1411200080598672, 0.13879810108005056, 0.13879810108005056, 0.006463259070189646, 0.006463259070189646]}\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(pos_id_list, d_model):\n",
    "    \"\"\"\n",
    "    pos_id为给定一个序列中的位置标识符，如0，1，2，3，...\n",
    "    这里给出一包含多个位置标识符的列表pos_id_list，要求对相应位置实现位置编码\n",
    "    d_model为位置编码维度，一般与使用模型中的隐藏层维度相关\n",
    "    pos_enc为输出，其中保留了全部的结果\n",
    "    \"\"\"\n",
    "    ##############################################\n",
    "    # TODO\n",
    "    # 实现正余弦位置编码\n",
    "    ##############################################\n",
    "\n",
    "    n = len(pos_id_list)\n",
    "\n",
    "    pos_enc = {}\n",
    "\n",
    "    for pos in pos_id_list:\n",
    "        pos_enc[pos] = [0] * 6\n",
    "        for i in range(0, d_model):\n",
    "            if i % 2 == 0: \n",
    "                pos_enc[pos][i] = np.sin(pos / (10000**(i / d_model)))\n",
    "            else:\n",
    "                pos_enc[pos][i] = np.cos(pos / (10000**((i - 1) / d_model)))\n",
    "    ##############################################\n",
    "    # 结束您的代码\n",
    "    ##############################################    \n",
    "    return pos_enc\n",
    "\n",
    "pos_id_list = [12,1,8,9,3]\n",
    "d_model = 6\n",
    "pos_enc = positional_encoding(pos_id_list, d_model)\n",
    "\n",
    "print(pos_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvM2_wOMlyOE"
   },
   "source": [
    "Task3：调用transformers库，实现对给定文本的逐词编码（embedding）。\n",
    "其中涉及词嵌入（Word Embedding）技术，其将离散的词语表示转换为连续的向量表示。\n",
    "给定文本：\n",
    "\n",
    "text1=\"This book is very interesting, I recommend it to you.\"\n",
    "\n",
    "text2=\"Having a cup of coffee in the morning can be refreshing.\"\n",
    "\n",
    "text3=\"He is learning programming and making rapid progress.\"\n",
    "\n",
    "\n",
    "你需要：1）将文本转为一个word的list；2）调用transformers库的AutoTokenizer和AutoModel等加载模型实现逐词编码\n",
    "\n",
    "除代码实现外，请简要说明所使用的词嵌入模型的基本原理。可探索对于中文句子的编码方式（会需要用到jieba等库实现中文分词）\n",
    "\n",
    "以下为用gensim库的一个简单示例说明流程。最终你需要给出你实现的代码以及结果并做必要的说明和分析。\n",
    "\n",
    "回答：\n",
    "+ BERT（Bidirectional Encoder Representations from Transformers）模型使用了两种嵌入技术：词嵌入（Word Embeddings）和位置嵌入（Position Embeddings）。这些嵌入技术共同构成了BERT模型的输入表示。\n",
    "    1. 词嵌入（Word Embeddings）：BERT使用了基于上下文的词嵌入技术。在预训练阶段，BERT模型首先使用WordPiece或者Byte Pair Encoding（BPE）等子词分词方法将单词划分成子词单元。然后，每个子词单元都被映射到一个向量空间中的固定维度的词嵌入向量。这些词嵌入向量是模型学习到的，能够捕捉单词的语义信息。\n",
    "    2. 位置嵌入（Position Embeddings）：由于BERT模型是基于Transformer结构的，它不像循环神经网络（RNN）那样具有自然的位置信息。因此，为了使模型能够理解输入序列的顺序信息，BERT模型引入了位置嵌入。位置嵌入是一种表示输入序列中每个位置的向量，它们通过正弦和余弦函数的周期性编码来表征不同位置的信息。这种编码方式能够捕捉到位置之间的相对关系，并为模型提供必要的位置信息。\n",
    "    \n",
    "    通过结合词嵌入和位置嵌入，BERT模型能够在预训练阶段学习到更丰富和更有意义的输入表示，从而在各种自然语言处理任务中取得较好的性能。\n",
    "\n",
    "+ 我实现的代码使用的是bert-base-uncased模型，是 Google 在 2018 年发布的 BERT 模型的一个版本，它是基于 Transformer 结构构建的双向编码器（Bidirectional Encoder）。因为无法连接huggingface，所以将模型下载到了本地。首先调用分词器对句子进行分词，然后将分词转化为词汇表索引。最后对每个词调用模型的输入词嵌入层，得到每个词的词嵌入向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hFXPTNmblyOE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\11910\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.65991211e-03  7.86132812e-02  1.09375000e-01  3.39843750e-01\n",
      " -2.08984375e-01  4.46777344e-02 -3.66210938e-02 -4.19921875e-02\n",
      "  1.92382812e-01  1.39648438e-01 -1.54296875e-01 -3.02734375e-02\n",
      " -1.13769531e-01  2.16796875e-01 -1.40625000e-01  3.83300781e-02\n",
      "  1.39648438e-01 -1.16210938e-01 -8.05664062e-02 -1.04003906e-01\n",
      "  7.08007812e-02  1.52587891e-02  1.06933594e-01  2.71484375e-01\n",
      "  1.93359375e-01  1.42578125e-01 -1.48315430e-02  1.94335938e-01\n",
      "  1.66992188e-01 -3.08593750e-01  3.06640625e-01  1.64062500e-01\n",
      " -3.57421875e-01 -5.70678711e-03 -1.37695312e-01  2.03857422e-02\n",
      "  1.31835938e-01  8.74023438e-02  7.47070312e-02 -3.27148438e-02\n",
      "  3.20312500e-01  9.71679688e-02  1.72851562e-01 -6.73828125e-02\n",
      " -1.81640625e-01  5.24902344e-03 -1.59179688e-01 -1.74560547e-02\n",
      "  6.73828125e-02  1.15722656e-01 -9.15527344e-03  1.99218750e-01\n",
      "  5.81054688e-02 -1.87500000e-01 -3.83300781e-02 -1.50390625e-01\n",
      " -1.98242188e-01 -3.53515625e-01 -4.90722656e-02 -1.47460938e-01\n",
      "  6.39648438e-02  1.34887695e-02  1.32812500e-01  1.05957031e-01\n",
      "  7.01904297e-04 -3.10546875e-01  1.12915039e-02  1.46484375e-01\n",
      " -2.87109375e-01  1.71875000e-01  7.17773438e-02  7.32421875e-02\n",
      "  1.23535156e-01  2.96875000e-01 -1.69921875e-01 -1.32812500e-01\n",
      "  6.78710938e-02  6.15234375e-02  9.47265625e-02  3.07617188e-02\n",
      "  3.88183594e-02  1.83105469e-02  4.23431396e-04 -8.39843750e-02\n",
      "  1.77734375e-01  2.12890625e-01  1.76757812e-01  2.01416016e-02\n",
      "  4.37011719e-02 -5.41992188e-02 -2.55126953e-02 -1.99218750e-01\n",
      "  5.07812500e-02 -9.27734375e-02  2.26562500e-01  1.68457031e-02\n",
      " -6.34765625e-02  9.47265625e-02 -6.88476562e-02  1.14135742e-02\n",
      "  1.01074219e-01  7.81250000e-03 -3.97949219e-02  2.87109375e-01\n",
      " -1.06933594e-01 -1.25000000e-01  1.41601562e-01 -1.94335938e-01\n",
      " -6.04248047e-03  1.69921875e-01 -8.98437500e-02  3.24707031e-02\n",
      "  9.57031250e-02  9.96093750e-02  3.33984375e-01 -9.91210938e-02\n",
      "  1.55273438e-01 -9.13085938e-02 -3.97949219e-02  7.12890625e-02\n",
      "  1.73339844e-02 -7.76367188e-02 -7.17773438e-02  2.43164062e-01\n",
      "  1.70898438e-02 -9.86328125e-02  8.30078125e-02 -1.58203125e-01\n",
      " -8.93554688e-02 -6.73828125e-02 -1.39648438e-01 -1.21582031e-01\n",
      "  8.64257812e-02 -5.05371094e-02 -1.19628906e-01  2.27539062e-01\n",
      " -9.37500000e-02 -1.30859375e-01 -1.15722656e-01 -8.48388672e-03\n",
      " -8.15429688e-02 -2.77343750e-01 -2.11914062e-01  2.17773438e-01\n",
      "  1.84326172e-02  2.03125000e-01 -1.09375000e-01 -2.13867188e-01\n",
      " -2.09960938e-01 -9.22851562e-02  2.81250000e-01 -1.57226562e-01\n",
      "  1.06445312e-01 -1.81640625e-01  3.44238281e-02 -1.05468750e-01\n",
      "  1.50146484e-02 -1.30859375e-01  5.12695312e-02  1.62353516e-02\n",
      "  7.03125000e-02  2.17285156e-02 -3.12500000e-02 -1.62109375e-01\n",
      "  5.29785156e-02 -4.56542969e-02  1.99218750e-01 -1.18164062e-01\n",
      "  5.32226562e-02 -5.15136719e-02 -1.25732422e-02  7.61718750e-02\n",
      " -1.47460938e-01 -3.88183594e-02  1.74804688e-01 -1.28906250e-01\n",
      "  1.53320312e-01  2.31933594e-02 -4.14062500e-01 -5.02929688e-02\n",
      " -2.45117188e-01 -1.97265625e-01  3.06396484e-02 -8.49609375e-02\n",
      "  5.56640625e-02 -2.05078125e-02 -1.12792969e-01 -4.95605469e-02\n",
      " -4.17480469e-02  1.39648438e-01  1.12792969e-01 -8.11767578e-03\n",
      "  7.03125000e-02  2.11914062e-01  5.24902344e-02 -7.61718750e-02\n",
      "  1.78710938e-01  2.27539062e-01 -1.10351562e-01  1.25976562e-01\n",
      "  3.27148438e-02 -1.78710938e-01 -4.27246094e-02 -1.50390625e-01\n",
      " -1.07421875e-01  1.99218750e-01 -2.07519531e-02  1.62109375e-01\n",
      "  1.77734375e-01 -2.77343750e-01  2.60009766e-02  3.04687500e-01\n",
      "  2.69531250e-01  2.27539062e-01 -7.47070312e-02 -3.14941406e-02\n",
      " -5.00488281e-03 -3.04687500e-01 -1.03515625e-01  3.39355469e-02\n",
      "  1.68457031e-02  1.55639648e-02 -2.55126953e-02  3.75976562e-02\n",
      "  1.43554688e-01 -2.61718750e-01  1.94335938e-01 -8.59375000e-02\n",
      "  1.42578125e-01  1.54418945e-02  7.47070312e-02  1.18408203e-02\n",
      "  5.10253906e-02 -2.91748047e-02  1.63085938e-01  3.03955078e-02\n",
      " -5.61523438e-03 -3.41796875e-01  5.20019531e-02  3.20434570e-03\n",
      " -3.78906250e-01 -1.67236328e-02 -2.17773438e-01 -2.31933594e-02\n",
      "  2.09960938e-01 -3.22265625e-02 -2.36816406e-02 -2.88085938e-02\n",
      " -8.17871094e-03  9.03320312e-02  1.29882812e-01  1.22070312e-01\n",
      "  9.15527344e-03 -4.10156250e-02 -7.86132812e-02 -2.68554688e-02\n",
      "  1.38671875e-01 -7.32421875e-02  6.93359375e-02  3.30078125e-01\n",
      "  2.88085938e-02  1.10351562e-01  2.47802734e-02  4.39453125e-01\n",
      "  5.64575195e-03 -1.47460938e-01 -1.53320312e-01 -1.31835938e-01\n",
      " -3.32031250e-02 -1.43432617e-02  1.22070312e-02  1.17675781e-01\n",
      "  2.50000000e-01 -2.59765625e-01  1.66015625e-01 -1.89453125e-01\n",
      " -3.18359375e-01 -8.15429688e-02  1.73339844e-02 -1.27929688e-01\n",
      "  2.18750000e-01  4.98046875e-02  5.61523438e-03 -7.61718750e-02\n",
      "  5.20019531e-02  1.13769531e-01 -1.19628906e-01  1.16699219e-01\n",
      " -5.17578125e-02 -2.50000000e-01 -3.32031250e-02  3.55468750e-01\n",
      " -2.05078125e-01  8.05664062e-02 -1.37695312e-01 -8.93554688e-02\n",
      "  1.99218750e-01  1.41601562e-01  1.07910156e-01  2.59765625e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.653 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '中国']\n",
      "[array([ 0.07910156, -0.28125   ,  0.06640625,  0.03759766, -0.02856445,\n",
      "        0.18457031,  0.09082031,  0.01098633, -0.04980469,  0.3828125 ,\n",
      "       -0.23730469,  0.10253906, -0.44726562,  0.26367188, -0.17480469,\n",
      "       -0.0168457 ,  0.05786133,  0.15527344, -0.28515625,  0.00872803,\n",
      "        0.08886719, -0.16113281,  0.49609375,  0.03222656, -0.22070312,\n",
      "       -0.40234375,  0.21386719,  0.09716797, -0.26953125, -0.11328125,\n",
      "       -0.19335938, -0.01843262,  0.14355469, -0.25390625, -0.20898438,\n",
      "       -0.19628906,  0.10058594,  0.171875  ,  0.08154297,  0.03063965,\n",
      "       -0.00318909, -0.20605469, -0.02075195,  0.02087402, -0.09375   ,\n",
      "       -0.24023438, -0.38085938, -0.22949219, -0.00386047,  0.03613281,\n",
      "       -0.45117188,  0.40429688,  0.25976562,  0.04150391, -0.11035156,\n",
      "        0.37109375, -0.265625  , -0.26367188,  0.24121094, -0.09082031,\n",
      "       -0.15332031,  0.44726562,  0.04736328,  0.22070312,  0.11425781,\n",
      "       -0.05029297,  0.06835938, -0.08984375,  0.4296875 ,  0.4453125 ,\n",
      "        0.11474609,  0.04760742, -0.07470703, -0.01446533, -0.46679688,\n",
      "       -0.35742188, -0.04736328, -0.19921875,  0.06347656,  0.125     ,\n",
      "       -0.0402832 ,  0.19238281, -0.14453125, -0.15332031,  0.12695312,\n",
      "        0.17285156,  0.15722656,  0.21582031,  0.125     , -0.09667969,\n",
      "       -0.42382812, -0.11621094, -0.06445312,  0.39257812,  0.0703125 ,\n",
      "       -0.27929688,  0.16015625,  0.11767578, -0.04541016, -0.18261719,\n",
      "        0.16992188,  0.23144531, -0.03222656, -0.08300781, -0.03808594,\n",
      "        0.38085938, -0.0402832 ,  0.27929688,  0.03759766,  0.265625  ,\n",
      "       -0.08203125, -0.31054688,  0.38671875, -0.06640625, -0.17578125,\n",
      "       -0.05566406, -0.24023438, -0.13867188,  0.15820312, -0.26953125,\n",
      "        0.25      ,  0.24121094, -0.27734375,  0.07421875,  0.33398438,\n",
      "        0.09472656, -0.46289062,  0.36523438,  0.17773438, -0.09521484,\n",
      "       -0.21484375,  0.43554688,  0.23730469,  0.29101562, -0.21582031,\n",
      "        0.27539062, -0.02172852, -0.14355469,  0.16308594,  0.16601562,\n",
      "        0.2421875 ,  0.24316406,  0.09375   ,  0.06982422, -0.03735352,\n",
      "       -0.0038147 ,  0.03100586,  0.296875  ,  0.02197266, -0.43945312,\n",
      "        0.34765625, -0.4140625 ,  0.09472656,  0.15234375,  0.1796875 ,\n",
      "       -0.20800781, -0.15527344,  0.2734375 ,  0.2421875 , -0.23144531,\n",
      "        0.06201172,  0.06152344,  0.13769531,  0.0456543 , -0.02490234,\n",
      "        0.0100708 ,  0.22265625, -0.03039551, -0.41992188,  0.13671875,\n",
      "       -0.12890625,  0.18945312, -0.09765625,  0.01025391, -0.14257812,\n",
      "        0.06176758,  0.17578125,  0.01660156, -0.17089844, -0.3203125 ,\n",
      "       -0.08740234,  0.1640625 ,  0.09570312, -0.11376953, -0.01348877,\n",
      "        0.05273438,  0.03613281, -0.40625   , -0.13085938,  0.04785156,\n",
      "       -0.296875  , -0.01104736, -0.31445312, -0.0456543 ,  0.26757812,\n",
      "        0.10498047, -0.08886719,  0.16308594,  0.01556396, -0.13964844,\n",
      "       -0.03051758, -0.03027344,  0.03442383,  0.01623535, -0.08300781,\n",
      "       -0.22460938,  0.10986328, -0.01220703, -0.32617188, -0.23339844,\n",
      "       -0.12207031, -0.11914062,  0.02856445,  0.25      ,  0.03393555,\n",
      "        0.03759766, -0.16210938,  0.03344727,  0.1484375 , -0.25195312,\n",
      "       -0.16210938,  0.10400391, -0.18164062,  0.07226562,  0.10546875,\n",
      "       -0.15136719, -0.12011719, -0.26953125, -0.14355469,  0.06396484,\n",
      "        0.10400391,  0.16699219, -0.15917969,  0.08740234,  0.140625  ,\n",
      "        0.00897217, -0.2734375 ,  0.01080322, -0.09033203,  0.01403809,\n",
      "       -0.046875  , -0.18261719,  0.21484375,  0.18457031,  0.125     ,\n",
      "       -0.11914062, -0.23828125,  0.55078125, -0.31054688, -0.21972656,\n",
      "        0.15625   ,  0.07324219,  0.09033203,  0.18652344,  0.2109375 ,\n",
      "        0.0324707 , -0.10595703, -0.11474609, -0.14746094, -0.07519531,\n",
      "        0.06054688,  0.28710938,  0.34960938, -0.34765625, -0.12792969,\n",
      "        0.0625    , -0.05761719, -0.11962891,  0.23730469, -0.03808594,\n",
      "       -0.21289062, -0.00157928, -0.21289062,  0.26953125,  0.30664062,\n",
      "        0.27148438, -0.3359375 ,  0.0177002 , -0.11523438,  0.23242188,\n",
      "       -0.0071106 ,  0.27929688, -0.20410156, -0.12060547,  0.046875  ,\n",
      "        0.01745605,  0.21875   ,  0.24804688,  0.04614258,  0.26757812,\n",
      "       -0.03125   ,  0.22753906, -0.11914062, -0.02697754,  0.00549316,\n",
      "       -0.04125977, -0.22265625, -0.14550781,  0.22265625,  0.40039062],\n",
      "      dtype=float32), array([-2.09960938e-02, -9.61914062e-02, -3.85742188e-02,  1.54418945e-02,\n",
      "        3.36914062e-02,  6.34765625e-02, -2.81982422e-02, -4.69970703e-03,\n",
      "       -1.08886719e-01,  1.28906250e-01, -5.76171875e-02,  1.57226562e-01,\n",
      "       -1.23046875e-01,  1.17187500e-02, -1.04003906e-01,  6.03027344e-02,\n",
      "       -3.36914062e-02,  3.51562500e-02, -2.34375000e-01,  6.29882812e-02,\n",
      "        2.83813477e-03, -7.95898438e-02,  1.74804688e-01,  1.46484375e-02,\n",
      "       -1.66992188e-01, -1.25976562e-01,  1.42578125e-01,  6.93359375e-02,\n",
      "       -6.78710938e-02, -9.03320312e-02, -7.37304688e-02,  6.49414062e-02,\n",
      "        6.68945312e-02, -3.93066406e-02, -8.15429688e-02, -8.83789062e-02,\n",
      "        1.96533203e-02,  1.78710938e-01,  5.05371094e-02, -1.00708008e-02,\n",
      "       -5.95703125e-02, -2.02941895e-03, -1.36718750e-01,  1.51367188e-01,\n",
      "        7.17773438e-02, -1.29882812e-01, -1.04492188e-01, -3.85742188e-02,\n",
      "        4.12597656e-02, -1.09252930e-02, -2.12890625e-01,  1.56250000e-01,\n",
      "        1.03027344e-01,  3.36914062e-02, -2.57568359e-02,  2.59765625e-01,\n",
      "       -8.49609375e-02, -1.46484375e-01,  1.66015625e-01, -1.01074219e-01,\n",
      "       -6.12792969e-02,  1.34765625e-01,  2.92968750e-02,  4.80957031e-02,\n",
      "       -6.83593750e-03, -7.27539062e-02,  1.11328125e-01, -8.64257812e-02,\n",
      "        1.49414062e-01,  1.32812500e-01,  1.56250000e-02, -7.27539062e-02,\n",
      "       -1.18652344e-01, -9.13085938e-02, -1.83105469e-02, -5.56640625e-02,\n",
      "       -3.83300781e-02, -1.62109375e-01,  9.52148438e-02, -9.81445312e-02,\n",
      "        7.37304688e-02, -8.15429688e-02, -1.13525391e-02, -5.34667969e-02,\n",
      "       -4.54101562e-02,  1.04980469e-01,  5.78613281e-02,  9.03320312e-02,\n",
      "        6.83593750e-02, -8.25195312e-02, -2.18750000e-01, -1.08398438e-01,\n",
      "       -3.88183594e-02,  1.57226562e-01,  7.01904297e-03,  1.47460938e-01,\n",
      "        7.08007812e-02, -1.78222656e-02,  2.85644531e-02, -8.64257812e-02,\n",
      "       -3.12500000e-02,  6.20117188e-02, -1.13769531e-01, -1.22558594e-01,\n",
      "        3.58581543e-03,  2.87109375e-01, -7.62939453e-04,  8.72802734e-03,\n",
      "       -2.65502930e-03,  2.04101562e-01, -1.22070312e-01, -4.61425781e-02,\n",
      "        2.08007812e-01, -6.49414062e-02,  1.09252930e-02, -1.25122070e-02,\n",
      "       -2.26562500e-01, -1.55273438e-01,  1.12792969e-01, -2.49023438e-01,\n",
      "        1.95312500e-02,  4.80957031e-02, -2.08007812e-01, -1.96838379e-03,\n",
      "        6.25000000e-02,  1.46484375e-01, -2.30468750e-01,  1.28906250e-01,\n",
      "        1.11083984e-02, -8.78906250e-02, -8.49609375e-02,  1.30859375e-01,\n",
      "        4.37011719e-02,  2.07031250e-01, -5.98144531e-02,  1.28906250e-01,\n",
      "       -1.32812500e-01, -1.04003906e-01,  4.98046875e-02,  3.24707031e-02,\n",
      "        1.84570312e-01,  6.12792969e-02, -4.39453125e-02,  5.17578125e-02,\n",
      "       -1.76239014e-03, -2.41699219e-02,  1.34765625e-01,  1.96289062e-01,\n",
      "       -9.61914062e-02, -5.27343750e-02,  9.08203125e-02, -2.23632812e-01,\n",
      "        1.29882812e-01,  4.93164062e-02,  7.86132812e-02, -1.43554688e-01,\n",
      "       -1.28906250e-01,  1.29882812e-01,  8.74023438e-02, -2.41210938e-01,\n",
      "        1.19628906e-02,  2.90527344e-02,  1.36718750e-01,  7.76367188e-02,\n",
      "        5.34667969e-02, -6.03027344e-02,  9.27734375e-02,  4.49218750e-02,\n",
      "       -2.46093750e-01,  5.34667969e-02, -4.45556641e-03,  4.29687500e-02,\n",
      "        3.29589844e-02, -1.08032227e-02,  1.40380859e-02,  1.29882812e-01,\n",
      "       -4.71191406e-02, -5.81054688e-02,  7.95898438e-02, -2.91015625e-01,\n",
      "       -1.59912109e-02,  4.15039062e-02,  3.93066406e-02, -9.52148438e-02,\n",
      "       -6.44531250e-02, -3.10058594e-02,  6.07910156e-02, -1.31835938e-01,\n",
      "       -1.73828125e-01, -1.19140625e-01, -2.78320312e-02,  8.15429688e-02,\n",
      "       -1.23046875e-01,  6.54296875e-02,  1.58203125e-01,  1.62353516e-02,\n",
      "       -5.71289062e-02, -5.15136719e-02,  4.27246094e-02, -2.96020508e-03,\n",
      "       -3.58886719e-02, -5.81054688e-02, -4.73632812e-02,  3.75976562e-02,\n",
      "       -7.03125000e-02, -1.74804688e-01,  6.22558594e-02, -3.58886719e-02,\n",
      "       -1.88476562e-01,  6.54296875e-02, -7.91015625e-02, -9.47265625e-02,\n",
      "        5.41992188e-02,  5.22460938e-02,  6.07299805e-03,  2.95410156e-02,\n",
      "       -1.06933594e-01, -1.95312500e-02,  1.37939453e-02, -4.95605469e-02,\n",
      "       -2.57568359e-02,  1.18164062e-01, -1.08886719e-01,  3.58886719e-02,\n",
      "       -4.44335938e-02, -4.90722656e-02, -1.10351562e-01, -6.34765625e-02,\n",
      "       -6.00585938e-02,  5.59082031e-02,  4.41894531e-02,  1.14257812e-01,\n",
      "       -1.65039062e-01,  1.23046875e-01,  1.15234375e-01,  7.51953125e-02,\n",
      "       -1.60156250e-01,  5.71289062e-02,  2.74658203e-03,  4.58984375e-02,\n",
      "        1.44042969e-02, -1.09863281e-01,  1.15356445e-02,  9.57031250e-02,\n",
      "        6.73828125e-02, -5.39550781e-02, -9.91210938e-02,  1.62109375e-01,\n",
      "       -4.90722656e-02, -1.35742188e-01,  2.06947327e-04,  6.64062500e-02,\n",
      "        7.32421875e-02,  3.10058594e-02,  3.73535156e-02,  4.93164062e-02,\n",
      "       -5.29785156e-02, -9.42382812e-02, -3.41796875e-02, -3.36914062e-02,\n",
      "        1.51367188e-01,  4.37011719e-02,  1.67968750e-01, -2.17773438e-01,\n",
      "        4.00390625e-02, -2.89306641e-02, -1.02050781e-01, -4.90722656e-02,\n",
      "        1.97265625e-01,  3.93066406e-02, -7.76367188e-02, -1.95312500e-01,\n",
      "       -1.22558594e-01,  1.42578125e-01,  1.11328125e-01,  1.09863281e-01,\n",
      "       -1.88476562e-01,  6.20117188e-02, -1.09863281e-01,  1.92382812e-01,\n",
      "       -2.16064453e-02,  5.56640625e-02, -1.11328125e-01, -5.59082031e-02,\n",
      "        9.03320312e-02,  4.39453125e-02,  9.22851562e-02, -5.27954102e-03,\n",
      "       -1.05468750e-01,  2.18750000e-01,  8.54492188e-02,  7.56835938e-02,\n",
      "       -1.04980469e-02,  2.34375000e-02,  4.56542969e-02, -1.43554688e-01,\n",
      "       -1.62109375e-01,  2.09960938e-02,  5.63964844e-02,  1.95312500e-01],\n",
      "      dtype=float32), array([ 0.04125977, -0.15722656, -0.01452637, -0.04931641, -0.07763672,\n",
      "       -0.09423828,  0.25      , -0.01696777, -0.00909424,  0.09570312,\n",
      "       -0.15527344,  0.17285156, -0.34375   ,  0.08056641, -0.35351562,\n",
      "        0.09960938, -0.1875    ,  0.22167969, -0.21875   ,  0.02453613,\n",
      "        0.15039062, -0.2734375 ,  0.29492188,  0.09619141, -0.12255859,\n",
      "        0.06347656,  0.21777344,  0.01782227, -0.21582031,  0.17675781,\n",
      "       -0.10449219,  0.03881836, -0.08837891, -0.11962891, -0.04907227,\n",
      "       -0.02539062,  0.04858398,  0.31640625,  0.22070312,  0.11132812,\n",
      "       -0.00585938, -0.10888672,  0.03198242,  0.10400391, -0.12890625,\n",
      "       -0.18164062, -0.32617188,  0.03686523, -0.12792969,  0.04174805,\n",
      "       -0.27148438,  0.3046875 ,  0.05395508,  0.12597656, -0.15625   ,\n",
      "        0.47851562, -0.3984375 , -0.29882812,  0.38085938, -0.10986328,\n",
      "       -0.04980469,  0.26171875,  0.05493164,  0.20898438, -0.08251953,\n",
      "        0.07226562,  0.13671875, -0.14257812,  0.44726562,  0.37304688,\n",
      "       -0.07910156,  0.02453613,  0.07666016,  0.0456543 , -0.11328125,\n",
      "       -0.28320312,  0.09814453,  0.00122833, -0.01513672,  0.04052734,\n",
      "        0.2109375 , -0.08789062,  0.04443359,  0.14648438, -0.10644531,\n",
      "        0.25      ,  0.10742188,  0.10449219,  0.06445312,  0.01171875,\n",
      "       -0.13867188, -0.17089844, -0.15039062,  0.15722656,  0.25390625,\n",
      "        0.03491211, -0.02368164, -0.0378418 , -0.02600098,  0.00927734,\n",
      "        0.09814453,  0.27148438,  0.02026367, -0.10986328, -0.12792969,\n",
      "        0.375     , -0.0534668 ,  0.11865234,  0.22265625,  0.23144531,\n",
      "        0.0062561 , -0.2265625 ,  0.10986328, -0.27148438,  0.0480957 ,\n",
      "        0.03710938, -0.13574219,  0.0267334 ,  0.18554688, -0.203125  ,\n",
      "        0.15917969,  0.14746094, -0.3828125 ,  0.20703125,  0.16992188,\n",
      "        0.08789062, -0.44726562,  0.31445312,  0.10009766, -0.25390625,\n",
      "       -0.1875    ,  0.49804688,  0.12402344,  0.22265625, -0.30273438,\n",
      "        0.29296875,  0.10351562, -0.25585938, -0.0456543 , -0.18945312,\n",
      "        0.23632812, -0.19433594,  0.0324707 ,  0.17480469,  0.15820312,\n",
      "       -0.19921875,  0.16113281,  0.19238281,  0.01495361, -0.20410156,\n",
      "        0.26171875, -0.31640625,  0.17089844, -0.19335938,  0.30859375,\n",
      "       -0.14746094, -0.3984375 ,  0.10546875,  0.26171875, -0.06933594,\n",
      "       -0.02819824,  0.11035156,  0.02941895, -0.09472656, -0.11132812,\n",
      "       -0.12402344,  0.125     ,  0.09375   , -0.31640625,  0.24902344,\n",
      "       -0.08056641,  0.02856445, -0.19238281, -0.03808594, -0.00488281,\n",
      "        0.01831055,  0.05932617, -0.08203125,  0.15332031, -0.390625  ,\n",
      "       -0.02539062,  0.04858398,  0.06347656, -0.13085938,  0.13867188,\n",
      "       -0.02612305, -0.00250244, -0.22167969, -0.18261719, -0.09960938,\n",
      "       -0.12597656,  0.0135498 , -0.43554688,  0.06494141,  0.02380371,\n",
      "        0.07226562, -0.09716797, -0.25195312,  0.05810547, -0.22167969,\n",
      "        0.0267334 , -0.03393555,  0.17871094,  0.09960938, -0.36914062,\n",
      "       -0.1484375 ,  0.28125   , -0.4296875 , -0.27148438, -0.04223633,\n",
      "       -0.19140625, -0.08837891,  0.17773438,  0.40429688,  0.20019531,\n",
      "        0.05664062,  0.03369141,  0.07763672,  0.25195312,  0.012146  ,\n",
      "       -0.20019531,  0.11865234, -0.34960938,  0.25195312,  0.11132812,\n",
      "       -0.16015625, -0.18457031, -0.17675781, -0.18066406,  0.02026367,\n",
      "        0.06542969,  0.17089844, -0.140625  ,  0.0703125 ,  0.06396484,\n",
      "       -0.05078125, -0.1015625 , -0.07568359, -0.02001953, -0.14453125,\n",
      "       -0.20605469, -0.421875  ,  0.05615234,  0.18066406,  0.1953125 ,\n",
      "       -0.22460938, -0.34375   ,  0.51171875, -0.29101562, -0.15917969,\n",
      "        0.01300049, -0.11767578,  0.15136719,  0.00622559,  0.07470703,\n",
      "        0.07763672, -0.25585938,  0.06347656, -0.04003906,  0.02844238,\n",
      "        0.18164062,  0.140625  ,  0.45703125, -0.30859375, -0.10986328,\n",
      "        0.09716797, -0.109375  , -0.07519531,  0.29492188,  0.09765625,\n",
      "       -0.11279297,  0.07275391, -0.001297  , -0.03613281, -0.01379395,\n",
      "        0.18164062, -0.34179688, -0.06835938, -0.05297852,  0.21972656,\n",
      "       -0.13085938,  0.17285156, -0.02416992, -0.02783203,  0.08935547,\n",
      "       -0.09912109,  0.16992188,  0.34570312, -0.04736328,  0.19238281,\n",
      "       -0.00793457,  0.02539062, -0.0456543 ,  0.1484375 ,  0.02197266,\n",
      "       -0.03808594, -0.20996094,  0.08789062,  0.0032196 ,  0.2265625 ],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "# 运行自己代码时注意安装相应库，可通过\n",
    "# !pip install jieba\n",
    "# 这样的形式在jupyter中安装\n",
    "# 模型下载如遇到huggingface连不上之类的问题，可手动下到本地或通过https://hf-mirror.com/等方法解决\n",
    "##############################################\n",
    "import gensim.downloader\n",
    "import jieba\n",
    "\n",
    "# 下载并加载预训练模型（遇到需要翻墙的问题可以自己去下载模型到本地）\n",
    "model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "# 对于无需分词的单个单词，可直接获得\"abandon\"的嵌入向量\n",
    "embedding = model['abandon']\n",
    "print(embedding)\n",
    "# 给定了文本句子\n",
    "text = \"我爱中国\"\n",
    "# 需要分词处理，得到一个list\n",
    "# 中文可能需要调用jieba等库\n",
    "tokens = jieba.lcut(text)\n",
    "print(tokens)\n",
    "# tokens = text.split()  # 英文文本分词为单词列表\n",
    "embeddings = [model[word] for word in tokens]  # 获取每个单词的嵌入向量\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "W3zp3--s9EeN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'book', 'is', 'very', 'interesting', ',', 'i', 'recommend', 'it', 'to', 'you', '.'], ['having', 'a', 'cup', 'of', 'coffee', 'in', 'the', 'morning', 'can', 'be', 'refreshing', '.'], ['he', 'is', 'learning', 'programming', 'and', 'making', 'rapid', 'progress', '.']]\n",
      "[[2023, 2338, 2003, 2200, 5875, 1010, 1045, 16755, 2009, 2000, 2017, 1012], [2383, 1037, 2452, 1997, 4157, 1999, 1996, 2851, 2064, 2022, 27150, 1012], [2002, 2003, 4083, 4730, 1998, 2437, 5915, 5082, 1012]]\n",
      "[tensor([[[-5.7095e-02,  1.5283e-02, -4.6868e-03,  ..., -3.2484e-03,\n",
      "           9.7317e-05,  9.4175e-03],\n",
      "         [ 3.5072e-03, -2.3005e-02,  3.8487e-02,  ..., -3.4511e-02,\n",
      "          -4.2856e-02, -9.5698e-03],\n",
      "         [-3.6044e-02, -2.4606e-02, -2.5735e-02,  ...,  3.3691e-03,\n",
      "          -1.8300e-03,  2.6855e-02],\n",
      "         ...,\n",
      "         [ 1.3139e-02,  8.1785e-03, -8.7239e-03,  ...,  1.5858e-02,\n",
      "          -7.8034e-03,  1.8179e-02],\n",
      "         [-3.6435e-02,  4.5340e-03,  3.0638e-02,  ...,  1.8992e-02,\n",
      "           1.1315e-03,  1.4183e-02],\n",
      "         [-2.0745e-02, -1.9870e-03, -1.1789e-02,  ...,  1.2760e-02,\n",
      "           2.0031e-02,  2.5873e-02]]], grad_fn=<EmbeddingBackward0>), tensor([[[-0.0153,  0.0311, -0.0112,  ...,  0.0242, -0.0740, -0.0121],\n",
      "         [ 0.0152,  0.0082,  0.0043,  ..., -0.0031, -0.0055,  0.0189],\n",
      "         [-0.0698, -0.0553,  0.0123,  ..., -0.0609, -0.0392,  0.0577],\n",
      "         ...,\n",
      "         [ 0.0182, -0.0076, -0.0458,  ...,  0.0136, -0.0478,  0.0276],\n",
      "         [-0.0085, -0.0667, -0.0243,  ..., -0.0370, -0.0429, -0.0046],\n",
      "         [-0.0207, -0.0020, -0.0118,  ...,  0.0128,  0.0200,  0.0259]]],\n",
      "       grad_fn=<EmbeddingBackward0>), tensor([[[-2.3479e-02, -1.6678e-02,  1.3104e-02,  ..., -3.4867e-02,\n",
      "          -4.8147e-02,  1.0689e-02],\n",
      "         [-3.6044e-02, -2.4606e-02, -2.5735e-02,  ...,  3.3691e-03,\n",
      "          -1.8300e-03,  2.6855e-02],\n",
      "         [-9.9035e-02, -3.9347e-02, -1.0866e-02,  ...,  7.3011e-03,\n",
      "          -1.9952e-02, -5.0741e-02],\n",
      "         ...,\n",
      "         [-3.0942e-05, -3.7216e-02, -8.2694e-03,  ..., -4.5258e-02,\n",
      "          -6.3191e-02, -6.0680e-02],\n",
      "         [-3.2381e-02, -1.1969e-02, -6.1464e-02,  ...,  1.3121e-02,\n",
      "          -6.3653e-02, -4.1025e-02],\n",
      "         [-2.0745e-02, -1.9870e-03, -1.1789e-02,  ...,  1.2760e-02,\n",
      "           2.0031e-02,  2.5873e-02]]], grad_fn=<EmbeddingBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "# 你可以在本地测试，或将你的代码写到这个单元格内，最终要有相应的结果\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert-base-uncased\") # 加载已下载到本地的模型（放在作业文件同一目录下）\n",
    "model = AutoModel.from_pretrained(\"./bert-base-uncased\")\n",
    "\n",
    "text1=\"This book is very interesting, I recommend it to you.\"\n",
    "text2=\"Having a cup of coffee in the morning can be refreshing.\"\n",
    "text3=\"He is learning programming and making rapid progress.\"\n",
    "\n",
    "texts = [text1, text2, text3]\n",
    "# print(texts)\n",
    "\n",
    "tokens = [tokenizer.tokenize(text) for text in texts] # 分词器进行分词\n",
    "print(tokens) # 打印结果\n",
    "\n",
    "words_id = [tokenizer.convert_tokens_to_ids(token) for token in tokens] # 转化为词汇表索引\n",
    "print(words_id)\n",
    "\n",
    "embed = model.get_input_embeddings() # 获取模型的输入词嵌入层\n",
    "\n",
    "embeddings = [embed(torch.tensor([word_id])) for word_id in words_id] # 对每个词汇表索引（即每个词）调用模型的输入词嵌入层，得到每个词的词嵌入向量。\n",
    "\n",
    "print(embeddings)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
